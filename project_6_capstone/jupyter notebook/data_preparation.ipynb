{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to help you prepare all data needed and save them to AWS S3.\n",
    "\n",
    "Steps:\n",
    "1. Download a kaggle.json(API token from Kaggle website) file with the API tokens in it. We need to move this file to â€“ ~/.kaggle/kaggle.json.\n",
    "2. Download file from Kaggle to your local box.\n",
    "3. Use Yelp API to download category data.\n",
    "4. Scrape weather data from wunderground.\n",
    "5. Copy local files to Amazon S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yelp dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the project\n",
    "project_path = '/mnt/data-ubuntu/Projects/data_engineering_projects/project_6_capstone/'\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(project_path + '/resource/project.cfg') # The project configure file is not included in the repo.\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['ACCESS_KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET_KEY']\n",
    "os.environ['REGION']=config['AWS']['REGION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your security, ensure that other users of your computer do not have read access to your credentials. \n",
    "# On Unix-based systems you can do this with the following command:\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an environment variable to specify the path your kaggle installed.\n",
    "os.environ['PATH'] = '/home/rick/anaconda3/envs/de_capstone/bin'\n",
    "!kaggle datasets download -d yelp-dataset/yelp-dataset -p {project_path + 'data/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping data set\n",
    "import zipfile\n",
    "with zipfile.ZipFile(project_path + 'data/yelp-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(project_path + 'data/yelp-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category data from yelp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working dir and import data_preparation.py\n",
    "os.chdir(project_path)\n",
    "from src.data_preparation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_category.csv is created in /mnt/data-ubuntu/Projects/data_engineering_projects/project_6_capstone/data/yelp-dataset\n"
     ]
    }
   ],
   "source": [
    "# Extract yelp business category data through yelp API\n",
    "api_key=config['YELP']['API_KEY']\n",
    "create_yelp_category(api_key, project_path + 'data/yelp-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Weather Data\n",
    "Get weather data based on the latitude and longitude. So we can analysze the relation between review and weather.\n",
    "\n",
    "After analyzing the data, AZ has the most number of businesses, while other stats don't have enough data.\n",
    "Thus, we will only focus on business in AZ in order to make a meaningful data model.\n",
    "\n",
    "And the reveiw started from 2004 to 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to get histical weather data:\n",
    "    1. Check missing values for column latitude and longitude.\n",
    "    2. Impute missing values if any.\n",
    "    3. Scrape weather data from www.wunderground.com based on latitude and longitude.\n",
    "    4. Save weather data by station as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets check out our location info in df_business.\n",
    "df_business = pd.read_json(project_path + 'data/yelp-dataset/yelp_academic_dataset_business.json', lines = True)\n",
    "df_business_address = df_business[df_business['state']=='AZ'].loc[:,['business_id',\n",
    "                                                                     'latitude',\n",
    "                                                                     'longitude']]\n",
    "# Replace field that's entirely space (or empty) with NaN\n",
    "df_business_address = df_business_address.replace(r'^\\s*$', np.nan, regex=True)\n",
    "# Trim the column city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check all missing values\n",
    "df_business_address.isnull().sum().sort_values(ascending = False)\n",
    "# There are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business_address.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will only keep one digits for latitude and longitude.(Reduce the number for searching)\n",
    "df_business_address['latit_s'] = df_business_address['latitude'].map(lambda x: round(x, 1))\n",
    "df_business_address['longi_s'] = df_business_address['longitude'].map(lambda x: round(x, 1))\n",
    "df_business_address.reset_index(drop = True, inplace = True)\n",
    "df_business_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was planning to use city and state to loop up the weather, but handling the missing values is a real headache.\n",
    "# mapquest api key. You can use the free 15,000 transactions per month\n",
    "# mapquest_api= config['MAPQUEST']['API_KEY']\n",
    "# for i, row in df_business_address.loc[:,['latitude', 'longitude']].iterrows():\n",
    "#     url = r'''\n",
    "#         http://open.mapquestapi.com/geocoding/v1/reverse?key={}&location={},{}\n",
    "#         &includeRoadMetadata=true\n",
    "#         &includeNearestIntersection=true \n",
    "#         '''.format(mapquest_api, row['latitude'], row['longitude'])\n",
    "       \n",
    "#     r = requests.get(url)\n",
    "#     df_business.at[i, 'city'] = r.json()['results'][0]['locations'][0]['adminArea5']\n",
    "#     df_business_address.at[i, 'city'] = r.json()['results'][0]['locations'][0]['adminArea5']\n",
    "# # Save the city and state as csv.\n",
    "# df_business_address.to_csv(project_path + 'data/yelp-dataset/address_imputation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look up weather data based on city and state**\n",
    "\n",
    "I built a scraper to get weather data(cities in AZ, from 2014-2019) from www.wunderground.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to look up the weather\n",
    "df_geo = df_business_address.loc[:,['latit_s', 'longi_s']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .reset_index(drop = True)\n",
    "df_geo.to_csv(project_path + 'data/yelp-dataset/geo_location.csv')\n",
    "# We need to find all historical weather info for these geo location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I tried to call function weather_scraper to downlaod the data but there was a bug that I can't fix when I use jupyter notebook to call the function. So I just run the data_preparation.py directly to scrape the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Makesure to change the chromedriver to executable\n",
    "# data_weather, df_city_state = weather_scraper(df_city_state, \n",
    "#                                               project_path + 'resource/chromedriver',\n",
    "#                                               '2004-01-01',\n",
    "#                                               '2020-01-01',\n",
    "#                                               config['WD']['API_KEY'])\n",
    "# df_city_state.to_csv(project_path + 'data/yelp-dataset')\n",
    "# # Parse weather date and save it as csv by station\n",
    "# parse_data(data_weather, project_path + 'data/weather-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_data = project_path + 'data/yelp-dataset'\n",
    "weather_data = project_path + 'data/weather-data'\n",
    "bucket = 'sparkify-de'\n",
    "# Create a bucket store data files\n",
    "!aws s3 mb s3://{bucket}\n",
    "# Copy your files to s3 bucket, this will take a while...\n",
    "!aws s3 cp {yelp_data} s3://{bucket}/test --recursive --exclude \"*\"  --include \"*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/yelp-dataset/geo_location.csv to s3://sparkify-de/yelp-dataset/geo_location.csv\n",
      "upload: ../data/yelp-dataset/yelp_category.csv to s3://sparkify-de/yelp-dataset/yelp_category.csv\n"
     ]
    }
   ],
   "source": [
    "# csv files for yelp_data\n",
    "!aws s3 cp {yelp_data} s3://{bucket}/yelp-dataset --recursive --exclude \"*\"  --include \"*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/weather-data/KVGT.csv to s3://sparkify-de/weather-data/KVGT.csv\n",
      "upload: ../data/weather-data/KPHX.csv to s3://sparkify-de/weather-data/KPHX.csv\n",
      "upload: ../data/weather-data/KIWA.csv to s3://sparkify-de/weather-data/KIWA.csv\n",
      "upload: ../data/weather-data/KLAS.csv to s3://sparkify-de/weather-data/KLAS.csv\n"
     ]
    }
   ],
   "source": [
    "# Copy weather data to s3\n",
    "!aws s3 cp {weather_data} s3://{bucket}/weather-data --recursive --exclude \"*\"  --include \"*.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
